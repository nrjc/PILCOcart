@inproceedings{pilco,
  title={{PILCO}: A model-based and data-efficient approach to policy search},
  author={M. Deisenroth and C. Rasmussen},
  booktitle={International Conference on Machine Learning (ICML 2011)},
  pages={465--472},
  address = {New York, NY, USA},
  year={2011},
}
# Under review as a conference paper at ICLR 2016.
@inproceedings{deepmind-continuous-control,
  title={Continuous control with deep reinforcement learning},
  author={T. Lillicrap and J. Hunt and A. Pritzel and N. Heess and T. Erez and Y. Tassa and D. Silver and D. Wierstra},
  booktitle={arXiv preprint},
  address={arXiv 1509.02971},
  year={2015},
}
@inproceedings{dallaire2009,
  title={Bayesian reinforcement learning in continuous {POMDP}s with {G}aussian processes},
  author={P. Dallaire and C. Besse and S. Ross and B. Chaib-Draa},
  booktitle={International Conference on Intelligent Robots and Systems (IROS 2009)},
  pages={2604--2609},
  year={2009},
  organization={IEEE}
}
@inproceedings{deisenroth2013,
  title={Solving nonlinear continuous state-action-observation {POMDP}s for mechanical systems with {G}aussian noise},
  author={M. Deisenroth and J. Peters},
  booktitle={European Workshop on Reinforcement Learning (EWRL 2012)},
  year={2012},
}
@phdthesis{mchutchon2014,
  title={Nonlinear modelling and control using Gaussian processes},
  chapter={3.5},
  author={A. McHutchon},
  year={2014},
  school={Department of Engineering, University of Cambridge},
}
@article{bayesFilters,
  title={{GP}-{B}ayes{F}ilters: {B}ayesian filtering using {G}aussian process prediction and observation models},
  author={J. Ko and D. Fox},
  journal={Autonomous Robots},
  volume={27},
  number={1},
  pages={75--90},
  year={2009},
  publisher={Springer},
}
@inproceedings{particleMethods,
  title={Bayesian reinforcement learning in continuous {POMDP}s with application to robot navigation},
  author={S. Ross and B. Chaib-Draa and J. Pineau},
  booktitle={International Conference on Robotics and Automation (ICRA 2008)},
  pages={2845--2851},
  year={2008},
  organization={IEEE},
}
@phdthesis{duff2002-phd,
  title={Optimal Learning: Computational procedures for {B}ayes-adaptive {M}arkov decision processes},
  author={M. Duff},
  year={2002},
  school={Department of Computer Science, University of Massachusetts Amherst},
}
@article{pomdp,
  title={The optimal control of partially observable {M}arkov processes over a finite horizon},
  author={R. Smallwood and E. Sondik},
  journal={Operations Research},
  volume={21},
  number={5},
  pages={1071--1088},
  year={1973},
}
@article{beliefMDPs,
  title={Planning and acting in partially observable stochastic domains},
  author={L. Kaelbling and M. Littman and A. Cassandra},
  journal={Artificial intelligence},
  volume={101},
  number={1},
  pages={99--134},
  year={1998},
  publisher={Elsevier}
}
@inproceedings{boone1997efficient,
  title={Efficient reinforcement learning: Model-based acrobot control},
  author={Boone, Gary},
  booktitle={Robotics and Automation, 1997. Proceedings., 1997 IEEE International Conference on},
  volume={1},
  pages={229--234},
  year={1997},
  organization={IEEE}
}
@inproceedings{atkeson1997,
  title={A comparison of direct and model-based reinforcement learning},
  author={Atkeson, Christopher G and Santamaria, Juan Carlos},
  booktitle={In International Conference on Robotics and Automation},
  year={1997},
  organization={Citeseer},
}
@inproceedings{van2012efficient,
  title={Efficient Approximate Value Iteration for Continuous Gaussian POMDPs.},
  author={Van Den Berg, Jur and Patil, Sachin and Alterovitz, Ron},
  booktitle={AAAI},
  year={2012}
}
@inproceedings{webb2014online,
  title={Online parameter estimation via real-time replanning of continuous Gaussian POMDPs},
  author={Webb, David J and Crandall, Kyle L and van den Berg, Jan},
  booktitle={Robotics and Automation (ICRA), 2014 IEEE International Conference on},
  pages={5998--6005},
  year={2014},
  organization={IEEE}
}
@inproceedings{ross2008bayesian,
  title={Bayesian reinforcement learning in continuous {POMDP}s with application to robot navigation},
  author={Ross, Stephane and Chaib-Draa, Brahim and Pineau, Joelle},
  booktitle={Robotics and Automation, 2008. ICRA 2008. IEEE International Conference on},
  pages={2845--2851},
  year={2008},
  organization={IEEE}
}
@phdthesis{duff2002,
  title={Optimal Learning: Computational procedures for {B}ayes-adaptive {M}arkov decision processes},
  author={M. Duff},
  year={2002},
  school={Department of Computer Science, University of Massachusetts Amherst},
}
@article{poupart2006,
title = {An Analytic Solution to Discrete {B}ayesian Reinforcement Learning},
author = {Pascal Poupart and Nikos Vlassis and Jesse Hoey and Kevin Regan},
year = {2006},
journal={Proceedings of the 23rd international conference on Machine learning},
pages = {697--704},
}
